import os
import tempfile
import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializar o modelo de chat
chat = ChatOpenAI(
    api_key=os.getenv('OPENAI_API_KEY'), # Chave de API
    model='gpt-4o' # Modelo LLM a ser usado
)

def obter_base_vetores_dos_pdfs(arquivos):
    """Carrega o conte√∫do de m√∫ltiplos arquivos PDF usando LangChain, divide o texto em peda√ßos e cria uma base vetorial."""

    # Lista para armazenar todos os documentos carregados
    documentos = []

    # Cria arquivos tempor√°rios para cada PDF enviado
    for arquivo in arquivos:
        # Cria um arquivo tempor√°rio para salvar o conte√∫do do arquivo enviado
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as arquivo_temporario:
            arquivo_temporario.write(arquivo.getvalue())
            caminho_arquivo = arquivo_temporario.name # Obt√©m o caminho do arquivo tempor√°rio

        # Usa o PyPDFLoader do LangChain para carregar o PDF
        carregador = PyPDFLoader(caminho_arquivo)
        # Carrega o documento e adiciona √† lista de documentos
        documentos.extend(carregador.load())

        # Remove o arquivo tempor√°rio ap√≥s o carregamento
        os.unlink(caminho_arquivo)

    # Configura o divisor de texto em peda√ßos
    divisor_texto = RecursiveCharacterTextSplitter(
        separators=['\n\n', '\n', '.', ' ', ''], # Define o separador como uma quebra de linha
        chunk_size=500, # Define o tamanho de cada peda√ßo de texto
        chunk_overlap=200, # Define a sobreposi√ß√£o entre os peda√ßos
    )

    # Divide o texto do documento em peda√ßos
    documentos_divididos = divisor_texto.split_documents(documentos)

    # Configura o modelo de embeddings para gerar representa√ß√µes vetoriais
    modelo_embeddings = OpenAIEmbeddings()

    # Cria uma base vetorial persistente usando os textos em peda√ßos
    base_vetores = FAISS.from_documents(documentos_divididos, modelo_embeddings)
    return base_vetores

def montar_prompt(fragmentos, pergunta):
    """Monta manualmente o prompt com os fragmentos e o hist√≥rico de conversa."""

    template = """
    Use os trechos fornecidos para responder √† pergunta do usu√°rio de forma clara e concisa.
    Se necess√°rio, complemente a resposta utilizando o hist√≥rico do chat.
    Se n√£o souber a resposta com base nos trechos fornecidos e no hist√≥rico do chat, diga que n√£o sabe, sem tentar adivinhar ou inventar informa√ß√µes.
    Se poss√≠vel, seja direto e objetivo ao responder.

    ### Trechos:
    {fragmentos}

    ### Pergunta:
    {pergunta}
    """

    # Juntar todos os fragmentos em um √∫nico texto
    contexto = '\n'.join([f'{indice}. {fragmento.page_content}\n' for indice, fragmento in enumerate(fragmentos,1)])

    # Criar e formatar o prompt
    prompt = template.format(fragmentos=contexto, pergunta=pergunta)

    return prompt

def main():
    """Fun√ß√£o principal para configurar e executar a interface da aplica√ß√£o Streamlit."""
    # Inicializa o hist√≥rico de chat na sess√£o, se ainda n√£o existir
    if 'historico_chat' not in st.session_state:
        st.session_state.historico_chat = []
    # Inicializa a base de vetores na sess√£o, se ainda n√£o existir
    if 'base_vetores' not in st.session_state:
        st.session_state.base_vetores = None
    # Inicializa o estado de desabilitado do prompt se n√£o existir
    if 'prompt_sistema_desabilitado' not in st.session_state:
        st.session_state.prompt_sistema_desabilitado = False

    # Configura o t√≠tulo e o √≠cone da p√°gina
    st.set_page_config(page_title='Chat com arquivos PDF', page_icon='ü§ñ')
    st.title('Chat com arquivos PDF')

    # Configura a barra lateral para upload de arquivos e cria√ß√£o de persona
    with st.sidebar:
        # Adicionar um campo para criar um prompt de sistema
        st.header('üé≠ Persona do Chatbot')
        prompt_sistema = st.text_area(
            'Defina o comportamento do bot aqui:',
            placeholder='Ex.: Voc√™ √© um assistente especializado em an√°lise de dados financeiros.',
            help='Insira um prompt para personalizar a persona do chatbot.',
            disabled=st.session_state.prompt_sistema_desabilitado
        )

        # Cabe√ßalho das configura√ß√µes
        st.header('üìÅ Upload de Documentos')
        # Permite envio de arquivos PDF
        arquivos_pdfs = st.file_uploader(
            'Selecione os arquivos PDF',
            type='pdf',
            accept_multiple_files=True,
            help='Voc√™ pode fazer upload de m√∫ltiplos arquivos PDF'
        )

        # Se os arquivos foram enviados e o bot√£o foi pressionado
        if arquivos_pdfs:
            if st.button('Processar PDFs', use_container_width=True):
                # Mostra spinner durante processamento
                with st.spinner('Processando documentos...'):
                    # Adiciona o prompt do sistema primeiro se existir e n√£o tiver sido adicionado ainda
                    if prompt_sistema and not any(isinstance(m, SystemMessage) for m in st.session_state.historico_chat):
                        st.session_state.historico_chat.insert(0, SystemMessage(content=prompt_sistema))
                        # Desabilita o prompt ap√≥s processar
                        st.session_state.prompt_sistema_desabilitado = True

                    # Inicializa o hist√≥rico de chat com a primeira mensagem do bot
                    st.session_state.historico_chat.append(AIMessage(content='Ol√°, me fa√ßa perguntas a respeito do conte√∫do carregado'))
                    # Processa o PDF e gera a base vetorial
                    st.session_state.base_vetores = obter_base_vetores_dos_pdfs(arquivos_pdfs)

                # Mostra mensagem de sucesso ap√≥s o processamento
                st.success('Documentos processados com sucesso!')

    if st.session_state.base_vetores is not None:
        # Exibe o hist√≥rico do chat na interface
        for mensagem in st.session_state.historico_chat:
            if isinstance(mensagem, AIMessage): # Mensagem do chatbot
                with st.chat_message('ai'):
                    st.write(mensagem.content)
            elif isinstance(mensagem, HumanMessage): # Mensagem do usu√°rio
                with st.chat_message('human'):
                    st.write(mensagem.content)

        # Captura a entrada do usu√°rio no chat
        pergunta = st.chat_input('Digite sua mensagem aqui...')

        # Processa a mensagem do usu√°rio e gera resposta
        if pergunta is not None and pergunta != '':
            # Exibir a pergunta do usu√°rio no chat
            with st.chat_message('human'):
                st.write(pergunta)

            # Cria uma mensagem no chat para o assistente
            with st.chat_message('ai'):
                placeholder = st.empty()

                # Exibe uma mensagem tempor√°ria no chat enquanto os documentos relevantes s√£o recuperados com base na pergunta do usu√°rio.
                placeholder.write('Recuperando...')

                # Recuperar documentos relevantes com base na pergunta usando o banco vetorial
                documentos_relevantes = st.session_state.base_vetores.max_marginal_relevance_search(pergunta, k=3, fetch_k=10)

                # Exibe uma mensagem tempor√°ria no chat indicando que o modelo est√° processando a resposta com base nos fragmentos recuperados.
                placeholder.write('Gerando resposta...')

                # Montar o prompt com os fragmentos
                prompt = montar_prompt(documentos_relevantes, pergunta)

                # Adiciona o prompt com os trechos e a pergunta ao hist√≥rico
                st.session_state.historico_chat.append(HumanMessage(content=prompt))

                # Obt√©m resposta do modelo considerando o hist√≥rico
                resposta = chat.stream(st.session_state.historico_chat)

                # Inicializa uma string para armazenar a resposta completa
                resposta_completa = ''

                # Acumula as partes da resposta
                for parte in placeholder.write_stream(resposta):
                    resposta_completa += parte # Acumula o texto gerado

            # Limpa o hist√≥rico antes de adicionar a resposta, removendo o prompt montado
            st.session_state.historico_chat.pop() # Remove a √∫ltima, que seria o prompt montado
            st.session_state.historico_chat.append(HumanMessage(content=pergunta)) # Adiciona apenas a pergunta ao hist√≥rico
            st.session_state.historico_chat.append(AIMessage(content=resposta_completa)) # Adicionar a resposta do modelo ao hist√≥rico de mensagens

# Executa a aplica√ß√£o se o script for chamado diretamente
if __name__ == '__main__':
    main()
