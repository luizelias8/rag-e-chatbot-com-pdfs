import os
import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from dotenv import load_dotenv
from PyPDF2 import PdfReader

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializar o modelo de chat
chat = ChatOpenAI(
    api_key=os.getenv('OPENAI_API_KEY'), # Chave de API
    model='gpt-4-1106-preview', # Modelo LLM a ser usado
    temperature=0.2, # Baixa temperatura para respostas mais precisas
    max_tokens=500 # Limite de tokens na resposta
)

def extrair_texto_completo(arquivos):
    """Extrai o texto completo de todos os PDFs."""
    # Vari√°vel para armazenar o texto extra√≠do de todos os PDFs
    documento = ''

    # Itera sobre cada arquivo enviado
    for arquivo in arquivos:
        # Cria um leitor para o arquivo PDF
        leitor_pdf = PdfReader(arquivo)
        # Itera sobre cada p√°gina do PDF
        for pagina in leitor_pdf.pages:
            # Adiciona o texto extra√≠do √† vari√°vel
            documento += pagina.extract_text()
    return documento

def obter_base_vetores_dos_pdfs(arquivos):
    """Carrega o conte√∫do de m√∫ltiplos arquivos PDF, divide o texto em peda√ßos e cria uma base vetorial."""

    documento = extrair_texto_completo(arquivos)

    # Configura o divisor de texto em peda√ßos
    divisor_texto = CharacterTextSplitter(
        separator='\n', # Define o separador como uma quebra de linha
        chunk_size=500, # Define o tamanho de cada peda√ßo de texto
        chunk_overlap=200, # Define a sobreposi√ß√£o entre os peda√ßos
        length_function=len # Usa o comprimento do texto para controle de tamanho
    )

    # Divide o texto do documento em peda√ßos
    pedacos_documento = divisor_texto.split_text(documento)

    # Configura o modelo de embeddings para gerar representa√ß√µes vetoriais
    modelo_embeddings = OpenAIEmbeddings()

    # Cria uma base vetorial persistente usando os textos em peda√ßos
    base_vetores = FAISS.from_texts(pedacos_documento, modelo_embeddings)
    return base_vetores, documento

def detectar_pedido_resumo(pergunta):
    """Detecta se a pergunta do usu√°rio √© um pedido de resumo."""
    palavras_chave_resumo = [
        'resumo', 'resuma', 'resumir', 'sintetizar', 's√≠ntese',
        'sumarizar', 'sum√°rio', 'overview', 'principais pontos'
    ]
    return any(palavra in pergunta.lower() for palavra in palavras_chave_resumo)

def montar_prompt_resumo(texto_completo, instrucoes_usuario):
    """Monta o prompt para gera√ß√£o de resumo, incorporando as instru√ß√µes espec√≠ficas do usu√°rio."""

    template = """
    Crie um resumo do documento fornecido seguindo as instru√ß√µes espec√≠ficas do usu√°rio.

    ### Instru√ß√µes do Usu√°rio:
    {instrucoes}

    ### Diretrizes Gerais (caso n√£o conflitem com as instru√ß√µes do usu√°rio):
    1. Manter a sequ√™ncia l√≥gica do documento original
    2. Garantir que as informa√ß√µes mais relevantes sejam inclu√≠das
    3. Manter a clareza e coes√£o do texto

    ### Documento:
    {texto}

    Por favor, gere um resumo seguindo as instru√ß√µes acima.
    """
    return template.format(
        instrucoes=instrucoes_usuario,
        texto=texto_completo
    )

def montar_prompt_rag(fragmentos, pergunta):
    """Monta manualmente o prompt com os fragmentos e o hist√≥rico de conversa."""

    template = """
    Use os trechos fornecidos para responder √† pergunta do usu√°rio de forma clara e concisa.
    Se necess√°rio, complemente a resposta utilizando o hist√≥rico do chat.
    Se n√£o souber a resposta com base nos trechos fornecidos e no hist√≥rico do chat, diga que n√£o sabe, sem tentar adivinhar ou inventar informa√ß√µes.
    Se poss√≠vel, seja direto e objetivo ao responder.

    ### Trechos:
    {fragmentos}

    ### Pergunta:
    {pergunta}
    """

    # Juntar todos os fragmentos em um √∫nico texto
    contexto = '\n'.join([f'{indice}. {fragmento.page_content}\n' for indice, fragmento in enumerate(fragmentos,1)])

    # Criar e formatar o prompt
    prompt = template.format(fragmentos=contexto, pergunta=pergunta)

    return prompt

def main():
    """Fun√ß√£o principal para configurar e executar a interface da aplica√ß√£o Streamlit."""
    # Inicializa o hist√≥rico de chat na sess√£o, se ainda n√£o existir
    if 'historico_chat' not in st.session_state:
        st.session_state.historico_chat = []
    # Inicializa a base de vetores na sess√£o, se ainda n√£o existir
    if 'base_vetores' not in st.session_state:
        st.session_state.base_vetores = None
    # Inicializa o texto completo na sess√£o, se ainda n√£o existir
    if 'texto_completo' not in st.session_state:
        st.session_state.texto_completo = None
    # Inicializa o estado de desabilitado do prompt se n√£o existir
    if 'prompt_sistema_desabilitado' not in st.session_state:
        st.session_state.prompt_sistema_desabilitado = False

    # Configura o t√≠tulo e o √≠cone da p√°gina
    st.set_page_config(page_title='Chat com arquivos PDF', page_icon='ü§ñ')
    st.title('Chat com arquivos PDF')

    # Configura a barra lateral para upload de arquivos e cria√ß√£o de persona
    with st.sidebar:
        # Adicionar um campo para criar um prompt de sistema
        st.header('üé≠ Persona do Chatbot')
        prompt_sistema = st.text_area(
            'Defina o comportamento do bot aqui:',
            placeholder='Ex.: Voc√™ √© um assistente especializado em an√°lise de dados financeiros.',
            help='Insira um prompt para personalizar a persona do chatbot.',
            disabled=st.session_state.prompt_sistema_desabilitado
        )

        # Cabe√ßalho das configura√ß√µes
        st.header('üìÅ Upload de Documentos')
        # Permite envio de arquivos PDF
        arquivos_pdfs = st.file_uploader(
            'Selecione os arquivos PDF',
            type='pdf',
            accept_multiple_files=True,
            help='Voc√™ pode fazer upload de m√∫ltiplos arquivos PDF'
        )

        # Se os arquivos foram enviados e o bot√£o foi pressionado
        if arquivos_pdfs:
            if st.button('Processar PDFs', use_container_width=True):
                # Mostra spinner durante processamento
                with st.spinner('Processando documentos...'):
                    # Adiciona o prompt do sistema primeiro se existir e n√£o tiver sido adicionado ainda
                    if prompt_sistema and not any(isinstance(m, SystemMessage) for m in st.session_state.historico_chat):
                        st.session_state.historico_chat.insert(0, SystemMessage(content=prompt_sistema))
                        # Desabilita o prompt ap√≥s processar
                        st.session_state.prompt_sistema_desabilitado = True

                    # Inicializa o hist√≥rico de chat com a primeira mensagem do bot
                    st.session_state.historico_chat.append(AIMessage(content='Ol√°, sou um bot. Como posso ajudar?'))
                    # Processa os PDFs, gera a base vetorial e texto completo dos documentos
                    st.session_state.base_vetores, st.session_state.texto_completo = obter_base_vetores_dos_pdfs(arquivos_pdfs)

                # Mostra mensagem de sucesso ap√≥s o processamento
                st.success('Documentos processados com sucesso!')

    if st.session_state.base_vetores is not None:
        # Captura a entrada do usu√°rio no chat
        pergunta = st.chat_input('Digite sua mensagem aqui...')
        # Processa a mensagem do usu√°rio e gera resposta
        if pergunta is not None and pergunta != '':
            # Verifica se √© um pedido de resumo
            if detectar_pedido_resumo(pergunta):
                prompt = montar_prompt_resumo(st.session_state.texto_completo, pergunta)
            else:
                documentos_relevantes = st.session_state.base_vetores.similarity_search(pergunta, k=3) # Recuperar documentos relevantes com base na pergunta
                prompt = montar_prompt_rag(documentos_relevantes, pergunta) # Montar o prompt com os fragmentos

            # Adiciona o prompt com os trechos e a pergunta ao hist√≥rico
            st.session_state.historico_chat.append(HumanMessage(content=prompt))

            # Exibir um spinner enquanto o modelo gera a resposta
            with st.spinner('Gerando resposta...'):
                resposta = chat.invoke(st.session_state.historico_chat) # Obter a resposta do modelo

            # Limpa o hist√≥rico antes de adicionar a resposta, removendo o prompt montado
            st.session_state.historico_chat.pop() # Remove a √∫ltima, que seria o prompt montado
            st.session_state.historico_chat.append(HumanMessage(content=pergunta)) # Adiciona apenas a pergunta ao hist√≥rico
            st.session_state.historico_chat.append(AIMessage(content=resposta.content)) # Adicionar a resposta do modelo ao hist√≥rico de mensagens

        # Exibe o hist√≥rico do chat na interface
        for mensagem in st.session_state.historico_chat:
            if isinstance(mensagem, AIMessage): # Mensagem do chatbot
                with st.chat_message('ai'):
                    st.write(mensagem.content)
            elif isinstance(mensagem, HumanMessage): # Mensagem do usu√°rio
                with st.chat_message('human'):
                    st.write(mensagem.content)

# Executa a aplica√ß√£o se o script for chamado diretamente
if __name__ == '__main__':
    main()
