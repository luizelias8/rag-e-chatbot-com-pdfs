import os
import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from dotenv import load_dotenv
from PyPDF2 import PdfReader

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializar o modelo de chat
chat = ChatOpenAI(
    api_key=os.getenv('OPENAI_API_KEY'), # Chave de API
    model='gpt-4-1106-preview', # Modelo LLM a ser usado
    temperature=0.2, # Baixa temperatura para respostas mais precisas
    max_tokens=500 # Limite de tokens na resposta
)

def obter_base_vetores_dos_pdfs(arquivos):
    """Carrega o conte√∫do de m√∫ltiplos arquivos PDF, divide o texto em peda√ßos e cria uma base vetorial."""

    # Vari√°vel para armazenar o texto extra√≠do de todos os PDFs
    documento = ''

    # Itera sobre cada arquivo enviado
    for arquivo in arquivos:
        # Cria um leitor para o arquivo PDF
        leitor_pdf = PdfReader(arquivo)
        # Itera sobre cada p√°gina do PDF
        for pagina in leitor_pdf.pages:
            # Adiciona o texto extra√≠do √† vari√°vel
            documento += pagina.extract_text()

    # Configura o divisor de texto em peda√ßos
    divisor_texto = CharacterTextSplitter(
        separator='\n', # Define o separador como uma quebra de linha
        chunk_size=500, # Define o tamanho de cada peda√ßo de texto
        chunk_overlap=200, # Define a sobreposi√ß√£o entre os peda√ßos
        length_function=len # Usa o comprimento do texto para controle de tamanho
    )

    # Divide o texto do documento em peda√ßos
    pedacos_documento = divisor_texto.split_text(documento)

    # Configura o modelo de embeddings para gerar representa√ß√µes vetoriais
    modelo_embeddings = OpenAIEmbeddings()

    # Cria uma base vetorial persistente usando os textos em peda√ßos
    base_vetores = FAISS.from_texts(pedacos_documento, modelo_embeddings)
    return base_vetores

def verificar_se_eh_resumo(pergunta):
    """Verifica se a pergunta √© uma solicita√ß√£o de resumo."""
    palavras_chave_resumo = ['resumo', 'resuma', 'sintetiza', 'sumarize']
    return any(palavra in pergunta.lower() for palavra in palavras_chave_resumo)

def obter_documentos_relevantes(base_vetores, pergunta, eh_pedido_resumo=False):
    """Obt√©m documentos relevantes da base vetorial, com quantidade adaptativa."""
    # Se for um pedido de resumo, recupera mais documentos
    k = 10 if eh_pedido_resumo else 3
    return base_vetores.similarity_search(pergunta, k=k)

def montar_prompt(fragmentos, pergunta, eh_pedido_resumo=False):
    """Monta manualmente o prompt com os fragmentos e o hist√≥rico de conversa."""

    if eh_pedido_resumo:
        template = """
        Crie um resumo conciso e bem estruturado usando os trechos do documento fornecidos abaixo.
        Organize o resumo de forma coerente, conectando as ideias principais encontradas nos diversos trechos.
        Se perceber que h√° partes faltando ou desconexas, indique isso no resumo.

        ### Trechos:
        {fragmentos}

        ### Solicita√ß√£o:
        {pergunta}
        """
    else:
        template = """
        Use os trechos fornecidos para responder √† pergunta do usu√°rio de forma clara e concisa.
        Se necess√°rio, complemente a resposta utilizando o hist√≥rico do chat.
        Se n√£o souber a resposta com base nos trechos fornecidos e no hist√≥rico do chat, diga que n√£o sabe, sem tentar adivinhar ou inventar informa√ß√µes.
        Se poss√≠vel, seja direto e objetivo ao responder.

        ### Trechos:
        {fragmentos}

        ### Pergunta:
        {pergunta}
        """

    # Juntar todos os fragmentos em um √∫nico texto
    contexto = '\n'.join([f'{indice}. {fragmento.page_content}\n' for indice, fragmento in enumerate(fragmentos,1)])

    # Criar e formatar o prompt
    prompt = template.format(fragmentos=contexto, pergunta=pergunta)

    return prompt

def main():
    """Fun√ß√£o principal para configurar e executar a interface da aplica√ß√£o Streamlit."""
    # Inicializa o hist√≥rico de chat na sess√£o, se ainda n√£o existir
    if 'historico_chat' not in st.session_state:
        st.session_state.historico_chat = []
    # Inicializa a base de vetores na sess√£o, se ainda n√£o existir
    if 'base_vetores' not in st.session_state:
        st.session_state.base_vetores = None
    # Inicializa o estado de desabilitado do prompt se n√£o existir
    if 'prompt_sistema_desabilitado' not in st.session_state:
        st.session_state.prompt_sistema_desabilitado = False

    # Configura o t√≠tulo e o √≠cone da p√°gina
    st.set_page_config(page_title='Chat com arquivos PDF', page_icon='ü§ñ')
    st.title('Chat com arquivos PDF')

    # Configura a barra lateral para upload de arquivos e cria√ß√£o de persona
    with st.sidebar:
        # Adicionar um campo para criar um prompt de sistema
        st.header('üé≠ Persona do Chatbot')
        prompt_sistema = st.text_area(
            'Defina o comportamento do bot aqui:',
            placeholder='Ex.: Voc√™ √© um assistente especializado em an√°lise de dados financeiros.',
            help='Insira um prompt para personalizar a persona do chatbot.',
            disabled=st.session_state.prompt_sistema_desabilitado
        )

        # Cabe√ßalho das configura√ß√µes
        st.header('üìÅ Upload de Documentos')
        # Permite envio de arquivos PDF
        arquivos_pdfs = st.file_uploader(
            'Selecione os arquivos PDF',
            type='pdf',
            accept_multiple_files=True,
            help='Voc√™ pode fazer upload de m√∫ltiplos arquivos PDF'
        )

        # Se os arquivos foram enviados e o bot√£o foi pressionado
        if arquivos_pdfs:
            if st.button('Processar PDFs', use_container_width=True):
                # Mostra spinner durante processamento
                with st.spinner('Processando documentos...'):
                    # Adiciona o prompt do sistema primeiro se existir e n√£o tiver sido adicionado ainda
                    if prompt_sistema and not any(isinstance(m, SystemMessage) for m in st.session_state.historico_chat):
                        st.session_state.historico_chat.insert(0, SystemMessage(content=prompt_sistema))
                        # Desabilita o prompt ap√≥s processar
                        st.session_state.prompt_sistema_desabilitado = True

                    # Inicializa o hist√≥rico de chat com a primeira mensagem do bot
                    st.session_state.historico_chat.append(AIMessage(content='Ol√°, sou um bot. Como posso ajudar?'))
                    # Processa o PDF e gera a base vetorial
                    st.session_state.base_vetores = obter_base_vetores_dos_pdfs(arquivos_pdfs)

                # Mostra mensagem de sucesso ap√≥s o processamento
                st.success('Documentos processados com sucesso!')

    if st.session_state.base_vetores is not None:
        # Captura a entrada do usu√°rio no chat
        pergunta = st.chat_input('Digite sua mensagem aqui...')

        # Processa a mensagem do usu√°rio e gera resposta
        if pergunta is not None and pergunta != '':
            # Verifica se √© pedido de resumo
            eh_pedido_resumo = verificar_se_eh_resumo(pergunta)

            # Recuperar documentos relevantes com base na pergunta
            documentos_relevantes = obter_documentos_relevantes(
                st.session_state.base_vetores,
                pergunta,
                eh_pedido_resumo
            )

            # Montar o prompt com os fragmentos
            prompt = montar_prompt(documentos_relevantes, pergunta, eh_pedido_resumo)

            # Adiciona o prompt com os trechos e a pergunta ao hist√≥rico
            st.session_state.historico_chat.append(HumanMessage(content=prompt))

            # Exibir um spinner enquanto o modelo gera a resposta
            with st.spinner('Gerando resposta...'):
                resposta = chat.invoke(st.session_state.historico_chat) # Obter a resposta do modelo

            # Limpa o hist√≥rico antes de adicionar a resposta, removendo o prompt montado
            st.session_state.historico_chat.pop() # Remove a √∫ltima, que seria o prompt montado
            st.session_state.historico_chat.append(HumanMessage(content=pergunta)) # Adiciona apenas a pergunta ao hist√≥rico
            st.session_state.historico_chat.append(AIMessage(content=resposta.content)) # Adicionar a resposta do modelo ao hist√≥rico de mensagens

        # Exibe o hist√≥rico do chat na interface
        for mensagem in st.session_state.historico_chat:
            if isinstance(mensagem, AIMessage): # Mensagem do chatbot
                with st.chat_message('ai'):
                    st.write(mensagem.content)
            elif isinstance(mensagem, HumanMessage): # Mensagem do usu√°rio
                with st.chat_message('human'):
                    st.write(mensagem.content)

# Executa a aplica√ß√£o se o script for chamado diretamente
if __name__ == '__main__':
    main()
